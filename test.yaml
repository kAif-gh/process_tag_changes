{{- if .Values.sparkJobs.tepIdInjector -}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    {{- include "sdpcli.kafka.labels" . | nindent 4 }}
  name: {{ include "sdpcli.kafka.name" . }}
  namespace: {{ include "namespace" . }}
data:
  {{ include "sparkJobs.tepIdInjector.tepIdChangesFileName" . }}: |-
    sourceTepId,targetTepId
    1b3639fd-3f2b-4cfa-9ac0-68e3590b16e9,1b3639fd-3f2b-4cfa-9ac0-68e3590b9999
---
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  labels:
    {{- include "spark.application.labels" . | nindent 4 }}
  name: spk-delta-tepid-injector-wf
  namespace: {{ include "namespace" . }}
spec:
  entrypoint: spk-delta-tepid-injector
  podGC:
    strategy: OnWorkflowSuccess
  serviceAccountName: {{ include "argo.workflows.serviceAccount.name" . }}
  arguments:
    parameters:
      - name: num-cores
        value: "{{ .Values.sparkJobs.tepIdInjector.executor.cores }}"
      - name: memory
        value: "{{ .Values.sparkJobs.tepIdInjector.executor.memory }}"
      - name: instances
        value: "{{ .Values.sparkJobs.tepIdInjector.executor.instances }}"
      - name: num-cores-limits
        value: "{{ .Values.sparkJobs.tepIdInjector.executor.coreLimit }}"
      - name: date-list
        value: |
          2024-06-12
          2024-08-01
          2024-10-02
          2024-10-03
          2024-10-04
  templates:
    - name: spk-delta-tepid-injector
      parallelism: 1
      steps:
        - - name: generate-intervals
            template: gen-intervals-python
        - - name: spk-delta-tepid-injector
            template: spk-delta-tepid
        withParam: "{{`{{steps.generate-intervals.outputs.result}}`}}"
            arguments:
              parameters:
              - name: startDate
                value: "{{`{{item.start_date}}`}}"
              - name: endDate
                value: "{{`{{item.end_date}}`}}"
    - name: gen-intervals-python
      script:
        image: {{ .Values.sparkJobs.tepIdInjector.genImage }}
        command: [python]
        source: |
          import pandas as pd
          import json

          input_dates = """{{workflow.parameters.date-list}}""".splitlines()
          date_list = [pd.to_datetime(date.strip()) for date in input_dates]

          # check for duplicates
          if len(date_list) != len(set(date_list)):
              raise ValueError("Duplicate dates found in the input list.")

          intervals = []
          start_date = date_list[0]
          end_date = date_list[0]
          for i in range(1, len(date_list)):
              if (date_list[i] - date_list[i-1]).days == 1:
                  end_date = date_list[i]
              else:
                  intervals.append({
                      "start_date": start_date.strftime("%Y-%m-%d"),
                      "end_date": end_date.strftime("%Y-%m-%d")
                  })
                  start_date = date_list[i]
                  end_date = date_list[i]

          intervals.append({
              "start_date": start_date.strftime("%Y-%m-%d"),
              "end_date": end_date.strftime("%Y-%m-%d")
          })
          print(json.dumps(intervals, indent=4))
          json.dumps(intervals)
    - name: spk-delta-tepid
      inputs:
        parameters:
          - name: startDate
          - name: endDate
      resource:
        action: create
        failureCondition: status.applicationState.state == FAILED
        successCondition: status.applicationState.state == COMPLETED
        manifest: |
          apiVersion: sparkoperator.k8s.io/v1beta2
          kind: SparkApplication
          metadata:
            name: delta-tepid-{{inputs.parameters.endDate}}-processor
            namespace: tep
            labels:
              {{- include "spark.application.labels" . | nindent 12 }}
            namespace: {{ include "namespace" . }}
          spec:
            type: Scala
            mode: cluster
            sparkVersion: 3.5.0
            image: auroradevacr.azurecr.io/sdp/tep-spark-k8s:0.7.80-SNAPSHOT-feat-delta_tepid_processor
            imagePullPolicy: Always
            mainClass: com.equinor.ows.tep.jobs.scadaDeltaNewTepIdInjector
            mainApplicationFile: local:///tep-databricks/jars/jobs-fatjar-1.0.jar
            arguments:
              - /tep-databricks/conf/spark-k8s.conf
            sparkConf:
              spark.driver.extraJavaOptions: -Duser.timezone=UTC
              spark.executor.heartbeatInterval: 10s
              spark.metrics.conf: /tep-databrics/tep-conf/metrics.properties
              spark.network.timeout: 600s
              spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
              spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension
              spark.sql.session.timeZone: UTC
              spark.sql.streaming.metricsEnabled: "true"
              spark.sql.files.ignoreMissingFiles: "true"
              spark.sql.redaction.options.regex: "(?i)url|(?i)begin"
              spark.databricks.delta.schema.autoMerge.enabled: "true"
              spark.memory.storageFraction: "0.4"
              spark.metrics.appStatusSource.enabled: "true"
            restartPolicy:
              type: Never
            driver:
              coreLimit: "2"
              cores: 2
              memory: 8G
              env:
                - name: APPNAME
                  value: delta-tepid-processor
                - name: DELTA_READ_STREAMING
                  value: "false"
                - name: DELTA_DATA_TYPE
                  value: SCADA
                - name: DELTA_PATH
                  value: "abfss://enterprise@dlstepdbdev.dfs.core.windows.net/scada/{{ .Values.sparkJobs.tepIdInjector.tableName }}"
                - name: DELTA_OUT
                  value: "abfss://enterprise@dlstepdbdev.dfs.core.windows.net/scada/{{ .Values.sparkJobs.tepIdInjector.tableName }}"
                - name: START_DATE
                  value: "{{inputs.parameters.startDate}}"
                - name: END_DATE
                  value: "{{inputs.parameters.endDate}}"
                - name: TABLE_NAME
                  value: "{{ .Values.sparkJobs.tepIdInjector.tableName }}"
                - name: DELTA_COMPRESSION_TYPE
                  value: "zstd"
                - name: FILE_PATH
                  value: "/opt/spark/config/tep_id_mapping_sample.csv"  # Reference to the ConfigMap file location
              envFrom:
                - secretRef:
                    name: {{ include "kafka.kubernetesSecret.internal.name" . }}
                - secretRef:
                    name: {{ include "storageAccount.dataLakeStorage.azureKeyVaultSecret.name" . }}
                - secretRef:
                    name: {{ include "redis.name" . }}
              {{- include "spark.application.spec.driver" . | nindent 12 }}
              nodeSelector:
                node-restriction.aurora.equinor.com/purpose: spark
              serviceAccount: spark-dgraph-reader
              tolerations:
                - key: SparkOnly
                  operator: Equal
                  value: "true"
                  effect: NoSchedule
              volumeMounts:
                - name: tep-id-mapping-vol
                  mountPath: /opt/spark/config/
            executor:
              instances: {{workflow.parameters.instances}}
              coreLimit: "{{workflow.parameters.num-cores-limits}}"
              cores: {{workflow.parameters.num-cores}}
              memory: "{{workflow.parameters.memory}}"
              javaOptions: -Duser.timezone=UTC
              labels:
                WBS: E.RDI.10959
                app.kubernetes.io/instance: release-name
                app.kubernetes.io/managed-by: Helm
                app.kubernetes.io/name: tep
                app.kubernetes.io/part-of: storm-data-platform
                app.kubernetes.io/version: 0.0.1
                azure.workload.identity/use: "true"
                costallocationcode: e.rdi.10959
                costallocationtype: wbs
                helm.sh/chart: tep-1.0.2
                project: doggerbankdev
                subproject: tep
                version: 3.5.0
              nodeSelector:
                node-restriction.aurora.equinor.com/purpose: spark-executors
              serviceAccount: spark-dgraph-reader
              tolerations:
                - key: SparkExecutorsOnly
                  operator: Equal
                  value: "true"
                  effect: NoSchedule
              volumeMounts:
                - name: tep-id-mapping-vol
                  mountPath: /opt/spark/config/
            volumes:
              - name: tep-id-mapping-vol
                configMap:
                  name: tep-id-mapping-cm
